{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of the hypermarameters of MLP using GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Loading, selecting and normalizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = mnist.load_data()\n",
    "\n",
    "# Filtering the data getting only 0s and\n",
    "train_filter = np.where((y_train_full == 0) | (y_train_full == 1))\n",
    "\n",
    "X_train_full, y_train_full = X_train_full[train_filter], y_train_full[train_filter]\n",
    "\n",
    "# Normalize the pixel values\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "X_train_full = X_train_full.reshape(X_train_full.shape[0], -1)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_train_full, y_train_full, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the MLP before the optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=200, alpha=0.0001,\n",
    "                    solver='adam', verbose=10, random_state=42,\n",
    "                    learning_rate_init=0.01)\n",
    "\n",
    "mlp.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of the loss of each iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "loss_history = mlp.loss_curve_\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(range(len(loss_history))), y=loss_history, mode='lines', name='Training Loss', line=dict(color='royalblue')))\n",
    "\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='white', mirror=True, gridcolor='purple', gridwidth=0.5)\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='white', mirror=True, gridcolor='purple', gridwidth=0.5)\n",
    "fig.update_layout(\n",
    "    title=\"Training Loss Over Iterations\",\n",
    "    xaxis_title=\"Iterations\",\n",
    "    yaxis_title=\"Training Loss\",\n",
    "    paper_bgcolor = 'rgba(0,0,0,0)', \n",
    "    plot_bgcolor = 'rgba(0,0,0,0)',\n",
    "    font=dict(color=\"white\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genetic Algorithm methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# transformation of the 2D matrix to a list \n",
    "def flatten_weights_biaises(model):\n",
    "    weights = []\n",
    "    biaises = []\n",
    "    for weight, biais in zip(model.coefs_, model.intercepts_):\n",
    "        weights.extend(weight.flatten())\n",
    "        biaises.extend(biais)\n",
    "    return weights, biaises\n",
    "\n",
    "# optimization of the weights and biaises\n",
    "def set_weights(model, weights, biaises):\n",
    "    start_w = 0\n",
    "    start_b = 0\n",
    "    new_weights = []\n",
    "    new_biaises = []\n",
    "    \n",
    "    for weight, biais in zip(model.coefs_, model.intercepts_):\n",
    "        weight_shape = weight.shape\n",
    "        weight_size = np.prod(weight_shape)\n",
    "        end_w = start_w + weight_size\n",
    "        new_weight = np.array(weights[start_w:end_w]).reshape(weight_shape)\n",
    "        start_w += weight_size\n",
    "        \n",
    "        bias_shape = biais.shape\n",
    "        biais_size = np.prod(bias_shape)\n",
    "        end_b = start_b + biais_size\n",
    "        new_biais = np.array(biaises[start_b:end_b]).reshape(bias_shape)\n",
    "        \n",
    "        new_weights.append(new_weight)\n",
    "        new_biaises.append(new_biais)\n",
    "    \n",
    "    model.coefs_ = new_weights\n",
    "    model.intercepts_ = new_biaises\n",
    "\n",
    "def fitness_function(chromosome, X_train, y_train, X_val, y_val):\n",
    "    hidden_layer_size = int(chromosome[0])\n",
    "    learning_rate = np.abs(chromosome[1])\n",
    "    alpha = chromosome[2]\n",
    "    weights = chromosome[3]\n",
    "    biases = chromosome[4]\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=(hidden_layer_size,), learning_rate_init=learning_rate, alpha=alpha, max_iter=1, warm_start=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    set_weights(model, weights, biases)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "def initialize_population(pop_size, model):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        hls = model.hidden_layer_sizes[0]\n",
    "        learning_rate_init = model.learning_rate_init\n",
    "        alpha = model.alpha\n",
    "        weights, biases = flatten_weights_biaises(model)\n",
    "        chromosome = [hls, learning_rate_init, alpha, weights, biases]\n",
    "        population.append(chromosome)\n",
    "    return population\n",
    "\n",
    "def select_parents(population, fitnesses):\n",
    "    total_fitness = sum(fitnesses)\n",
    "    selection_probs = [fitness / total_fitness for fitness in fitnesses]\n",
    "    parents = random.choices(population, weights=selection_probs, k=len(population))\n",
    "    return parents\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    def multiply_nested_list(element, a):\n",
    "        result = []\n",
    "        if isinstance(element, list):\n",
    "            for item in element:\n",
    "                result.append(item * a)\n",
    "            return result\n",
    "        return element * a\n",
    "    \n",
    "    alpha  = random.random() # generates numbers between 0 and 1\n",
    "    child1 = [multiply_nested_list(p1, alpha) + multiply_nested_list(p2, 1 - alpha) for p1, p2 in zip(parent1, parent2)]\n",
    "    child2 = [multiply_nested_list(p1, 1 - alpha) + multiply_nested_list(p2, alpha) for p1, p2 in zip(parent1, parent2)]\n",
    "    \n",
    "    return child1, child2\n",
    "\n",
    "def mutate(population):\n",
    "    def mutate_chromosome(chromosome):\n",
    "        gene_index = random.randint(0, len(chromosome) - 1)\n",
    "        mutation_value = random.uniform(0, 0.1) \n",
    "        if isinstance(chromosome[gene_index], list):\n",
    "            index = random.randint(0, len(chromosome[gene_index]) - 1)\n",
    "            chromosome[gene_index][index] += mutation_value\n",
    "        else:\n",
    "            chromosome[gene_index] += mutation_value\n",
    "        \n",
    "        return chromosome\n",
    "    \n",
    "    chromosome_index = random.randint(0, len(population) - 1)\n",
    "    chromosome = population[chromosome_index]\n",
    "    mutated_chromosome = mutate_chromosome(chromosome)\n",
    "    population[chromosome_index] = mutated_chromosome  \n",
    "    return population\n",
    "                \n",
    "def genetic_algorithm(X_train, y_train, X_val, y_val, pop_size, generations):\n",
    "    model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=10, warm_start=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    population = initialize_population(pop_size, model)\n",
    "    fitness_history = []\n",
    "    best_chromosome_per_generation = []\n",
    "    for _ in range(generations):\n",
    "        fitnesses = [fitness_function(chromosome, X_train, y_train, X_val, y_val) for chromosome in population]\n",
    "        fitness_history.append(max(fitnesses))\n",
    "        best_params = population[fitnesses.index(max(fitnesses))]\n",
    "        best_chromosome_per_generation.append((best_params, 1 - max(fitnesses)))\n",
    "        parents = select_parents(population, fitnesses)\n",
    "        next_population = []\n",
    "        for i in range(0, len(parents)-1, 2):\n",
    "            parent1, parent2 = parents[i], parents[i + 1]\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            next_population.extend([child1, child2])\n",
    "        mutate(next_population)\n",
    "        population = next_population\n",
    "    best_chromosome = max(population, key=lambda chromo: fitness_function(chromo, X_train, y_train, X_val, y_val))\n",
    "    return best_chromosome, fitness_history, best_chromosome_per_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 5\n",
    "generations = 10\n",
    "\n",
    "best_solution, fitness_history, best_chromosome_per_generation= genetic_algorithm(X_train, y_train, X_val, y_val, pop_size, generations)\n",
    "print(\"Best solution:\", best_solution)\n",
    "\n",
    "for generation, (params, error) in enumerate(best_chromosome_per_generation):\n",
    "    print(f\"Generation {generation + 1}: Best Params: {params}, Error: {error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test dataset\n",
    "hidden_layer_size = int(best_solution[0])\n",
    "learning_rate_init = best_solution[1]\n",
    "alpha = best_solution[2]\n",
    "weights = best_solution[3]\n",
    "biaises = best_solution[4]\n",
    "\n",
    "final_model = MLPClassifier(hidden_layer_sizes=(hidden_layer_size,), learning_rate='constant', learning_rate_init=learning_rate_init, alpha=alpha, max_iter=200, warm_start=True)\n",
    "final_model.fit(X_train, y_train)\n",
    "set_weights(final_model, weights, biaises)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(range(len(fitness_history))), y=fitness_history, mode='lines', name='Fitness', line=dict(color='blue')))\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True, gridcolor='purple', gridwidth=0.5)\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True, gridcolor='purple', gridwidth=0.5)\n",
    "fig.update_layout(\n",
    "    title=\"Fitness Evolution Over Generations\",\n",
    "    xaxis_title=\"Generations\",\n",
    "    yaxis_title=\"Fitness\",\n",
    "    paper_bgcolor = 'rgba(0,0,0,0)', \n",
    "    plot_bgcolor = 'rgba(0,0,0,0)',\n",
    "    font=dict(color=\"white\"),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_per_generation = [error for _, error in best_chromosome_per_generation]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(range(len(errors_per_generation))), y=errors_per_generation, mode='lines+markers', name='Errors per generations', line=dict(color='violet'), marker=dict(symbol='circle', size=8, color='grey')))\n",
    "\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True, gridcolor='purple', gridwidth=0.5)\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True, gridcolor='purple', gridwidth=0.5)\n",
    "fig.update_layout(\n",
    "    title=\"Evolution of Minimum Classification Error Over Generations\",\n",
    "    xaxis_title=\"Generations\",\n",
    "    yaxis_title=\"Classification Error\",\n",
    "    paper_bgcolor = 'rgba(0,0,0,0)', \n",
    "    plot_bgcolor = 'rgba(0,0,0,0)',\n",
    "    font=dict(color=\"white\"),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Précision du MLP seul\n",
    "accuracy_mlp_seul = accuracy\n",
    "\n",
    "# Précision du MLP optimisé par AG\n",
    "accuracy_mlp_optimise = test_accuracy\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"Précision du MLP seul: {accuracy_mlp_seul:.4f}\")\n",
    "print(f\"Précision du MLP optimisé par AG: {accuracy_mlp_optimise:.4f}\")\n",
    "\n",
    "# Comparer les résultats\n",
    "if accuracy_mlp_optimise > accuracy_mlp_seul:\n",
    "    print(\"Le MLP optimisé par AG a une meilleure précision que le MLP seul.\")\n",
    "elif accuracy_mlp_optimise < accuracy_mlp_seul:\n",
    "    print(\"Le MLP seul a une meilleure précision que le MLP optimisé par AG.\")\n",
    "else:\n",
    "    print(\"Les deux modèles ont la même précision.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
